{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "GAT Edge Risk (robust-B)\n",
    "- ROAD: width 미사용(= w_per_lane 없음). 폭 관련은 lanes*3.5로만 파생 사용\n",
    "- Y: risk_score (0~1), log-space 회귀 (Y_SCALE)\n",
    "- Split: pair(u,v) 기준 GroupShuffleSplit (누수 방지)\n",
    "- Graph: 양방향(2E)로 메시지패싱, 학습/평가/저장은 앞 E(원본)만 사용\n",
    "- Loss: Huber(SmoothL1) + Quantile(원스케일) 혼합 + 동적잔차 가중\n",
    "- Calibration: Train-CV 기반 Piecewise (lin_low@log + iso_high@orig) + 스무스 블렌드\n",
    "- Safety: 캘리브가 검증 성능을 악화시키면 자동 비활성화(fallback to raw)\n",
    "\"\"\"\n",
    "\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GroupShuffleSplit, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# =========================\n",
    "# Paths / Config\n",
    "# =========================\n",
    "ROAD_CSV = \"data/마포_서대문_은평_도로망_최종_v2.csv\"      \n",
    "Y_CSV    = \"data/은마서_도로_위험도_calibrated.csv\"\n",
    "OUT_DIR  = \"outputs30\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Train\n",
    "EPOCHS        = 800\n",
    "PRINT_EVERY   = 80\n",
    "PATIENCE      = 140\n",
    "LR            = 3e-4\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "\n",
    "GAT_HIDDEN = 128\n",
    "GAT_HEADS = 8\n",
    "GAT_LAYERS = 3\n",
    "MLP_HIDDEN = 512\n",
    "GAT_DROPOUT = 0.3\n",
    "MLP_DROPOUT = 0.3\n",
    "\n",
    "# Split\n",
    "TEST_SIZE     = 0.10\n",
    "VAL_SIZE      = 0.10\n",
    "\n",
    "# Target transform\n",
    "Y_SCALE       = 4000.0\n",
    "\n",
    "# Quantile\n",
    "QL_TAU    = 0.90\n",
    "QL_LAMBDA = 0.35\n",
    "\n",
    "# Weights\n",
    "LEN_W, SIG_W, HI_W = 0.4, 0.2, 0.4\n",
    "W_MIN, W_MAX       = 0.1, 10.0\n",
    "RES_ALPHA, RES_BETA = 2.0, 1.0\n",
    "RES_CLIP_MINMAX     = (1.0, 8.0)\n",
    "\n",
    "# High-risk\n",
    "HI_PCTILE = 0.85\n",
    "HI_MULT   = 6.0\n",
    "   # (hotfix) 과가중 완화\n",
    "\n",
    "# Calibration blend\n",
    "BLEND_K       = 100.0 # (hotfix) 경계 과보정 완화\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "def to_pair_tuple(a, b):\n",
    "    a = abs(int(a)); b = abs(int(b))\n",
    "    return (a, b) if a <= b else (b, a)\n",
    "\n",
    "def progress_bar(curr, total, width=40):\n",
    "    done = int(width * curr / total)\n",
    "    return \"[\" + \"#\" * done + \"-\" * (width - done) + f\"] {curr:>3d}/{total}\"\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float); y_pred = np.asarray(y_pred, dtype=float)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    denom = np.clip(np.abs(y_true), 1e-12, None)\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "    return {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2, \"MAPE%\": mape}\n",
    "\n",
    "def y_to_t(y):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    return np.log1p(np.clip(y, 0.0, 1.0) * Y_SCALE).astype(np.float32)\n",
    "\n",
    "def t_to_y(t):\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    return np.clip(np.expm1(t) / Y_SCALE, 0.0, 1.0).astype(np.float32)\n",
    "\n",
    "def t_to_y_torch(t_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.clamp(torch.expm1(t_tensor) / Y_SCALE, min=0.0, max=1.0)\n",
    "\n",
    "def quantile_loss(y_true_torch, y_pred_torch, tau=0.9):\n",
    "    e = y_true_torch - y_pred_torch\n",
    "    return torch.mean(torch.maximum(tau*e, (tau-1)*e))\n",
    "\n",
    "def group_splits_by_pair(df, labeled_idx, test_size=0.10, val_size=0.10, seed=42):\n",
    "    \"\"\"pair(u,v) 기준으로 Train/Val/Test 인덱스 반환 (df의 row-index 기준).\"\"\"\n",
    "    lab_df = df.iloc[labeled_idx]\n",
    "    groups = lab_df[\"pair\"].astype(str).values\n",
    "\n",
    "    gss1 = GroupShuffleSplit(n_splits=1, test_size=(test_size+val_size), random_state=seed)\n",
    "    tr_rel, hold_rel = next(gss1.split(lab_df, groups=groups))\n",
    "\n",
    "    hold_df = lab_df.iloc[hold_rel]\n",
    "    hold_groups = hold_df[\"pair\"].astype(str).values\n",
    "    test_frac = test_size / (test_size + val_size)\n",
    "    gss2 = GroupShuffleSplit(n_splits=1, test_size=test_frac, random_state=seed+1)\n",
    "    va_rel, te_rel = next(gss2.split(hold_df, groups=hold_groups))\n",
    "\n",
    "    lab_train = labeled_idx[tr_rel]\n",
    "    lab_val   = labeled_idx[hold_rel[va_rel]]\n",
    "    lab_test  = labeled_idx[hold_rel[te_rel]]\n",
    "    return lab_train, lab_val, lab_test\n",
    "\n",
    "# =========================\n",
    "# 1) Load & Merge\n",
    "# =========================\n",
    "roads = pd.read_csv(ROAD_CSV)\n",
    "if not {\"u\",\"v\"}.issubset(roads.columns):\n",
    "    raise ValueError(\"ROAD_CSV에는 'u','v' 컬럼이 필요합니다.\")\n",
    "roads[\"pair\"] = [to_pair_tuple(u, v) for u, v in zip(roads[\"u\"], roads[\"v\"])]\n",
    "\n",
    "ys = pd.read_csv(Y_CSV)\n",
    "if \"risk_score\" not in ys.columns:\n",
    "    raise ValueError(\"Y_CSV에 'risk_score' 컬럼이 필요합니다.\")\n",
    "\n",
    "# 노드 컬럼 자동 감지\n",
    "if {\"u\",\"v\"}.issubset(ys.columns): au, av = \"u\",\"v\"\n",
    "elif {\"u_id\",\"v_id\"}.issubset(ys.columns): au, av = \"u_id\",\"v_id\"\n",
    "elif {\"node1\",\"node2\"}.issubset(ys.columns): au, av = \"node1\",\"node2\"\n",
    "else: raise ValueError(\"Y_CSV에서 노드쌍 컬럼(u,v / u_id,v_id / node1,node2) 미발견\")\n",
    "\n",
    "ys[\"pair\"] = [to_pair_tuple(a,b) for a,b in zip(ys[au], ys[av])]\n",
    "ys[\"risk_score\"] = ys[\"risk_score\"].astype(float).clip(0.0, 1.0)\n",
    "ys = ys.groupby(\"pair\", as_index=False)[\"risk_score\"].max()\n",
    "\n",
    "df = roads.merge(ys, on=\"pair\", how=\"left\").copy()\n",
    "print(f\"[INFO] roads={len(roads):,}, labeled_edges={df['risk_score'].notna().sum():,} \"\n",
    "      f\"({df['risk_score'].notna().mean()*100:.1f}% labeled)\")\n",
    "\n",
    "# =========================\n",
    "# 2) Edge features  (폭은 lanes*3.5만 사용)\n",
    "# =========================\n",
    "# 숫자 캐스팅\n",
    "for c in [\"length\",\"lanes\",\"avg_height\",\"avg_slope\",\"up_lanes\",\"down_lanes\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# 안전 처리\n",
    "if \"length\" in df.columns:\n",
    "    df[\"length\"] = df[\"length\"].clip(lower=0)\n",
    "    df[\"log_length\"] = np.log1p(df[\"length\"])\n",
    "else:\n",
    "    df[\"length\"] = 0.0\n",
    "    df[\"log_length\"] = 0.0\n",
    "\n",
    "if \"avg_slope\" in df.columns:\n",
    "    df[\"abs_slope\"] = np.abs(df[\"avg_slope\"])\n",
    "else:\n",
    "    df[\"abs_slope\"] = 0.0\n",
    "\n",
    "if {\"length\",\"avg_slope\"}.issubset(df.columns):\n",
    "    df[\"len_x_slope\"] = df[\"length\"] * np.abs(df[\"avg_slope\"])\n",
    "else:\n",
    "    df[\"len_x_slope\"] = 0.0\n",
    "\n",
    "# lanes 보정\n",
    "if \"lanes\" in df.columns:\n",
    "    df[\"lanes\"] = df[\"lanes\"].fillna(1).clip(1, 8)\n",
    "else:\n",
    "    df[\"lanes\"] = 1.0\n",
    "\n",
    "# width_len_ratio (폭은 lanes*3.5 가정), + 로그 파생\n",
    "est_width = df[\"lanes\"] * 3.5\n",
    "df[\"width_len_ratio\"] = est_width / (df[\"log_length\"] + 1e-6)\n",
    "df[\"log_width_len_ratio\"] = np.log1p(df[\"width_len_ratio\"])\n",
    "\n",
    "# 이진 컬럼 정리\n",
    "for c in [\"oneway\",\"bridge\",\"tunnel\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).round().astype(int)\n",
    "\n",
    "# 후보\n",
    "num_candidates = [\n",
    "    \"length\",\"lanes\",\"avg_height\",\"avg_slope\",\n",
    "    \"up_lanes\",\"down_lanes\",\n",
    "    \"log_length\",\"abs_slope\",\"len_x_slope\",\n",
    "    \"width_len_ratio\",\"log_width_len_ratio\",\n",
    "    \"oneway\",\"bridge\",\"tunnel\",\n",
    "]\n",
    "cat_candidates = [\"highway\",\"surface\"]\n",
    "\n",
    "# 결측/더미화\n",
    "edge_num_cols = [c for c in num_candidates if c in df.columns]\n",
    "for c in edge_num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(float)\n",
    "for c in edge_num_cols:\n",
    "    df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "edge_cat_cols = [c for c in cat_candidates if c in df.columns and df[c].dtype == object]\n",
    "for c in edge_cat_cols:\n",
    "    df[c] = df[c].astype(\"object\").fillna(\"Unknown\")\n",
    "    vc = df[c].value_counts(dropna=False)\n",
    "    rare = vc[vc < 20].index\n",
    "    df[c] = df[c].where(~df[c].isin(rare), other=\"Other\")\n",
    "\n",
    "if edge_cat_cols:\n",
    "    dummies = pd.get_dummies(df[edge_cat_cols], prefix=edge_cat_cols, dtype=np.float32)\n",
    "    df = pd.concat([df.drop(columns=edge_cat_cols), dummies], axis=1)\n",
    "\n",
    "edge_feat_cols = []\n",
    "edge_feat_cols += edge_num_cols\n",
    "edge_feat_cols += [c for c in df.columns if any(c.startswith(p+\"_\") for p in cat_candidates)]\n",
    "if len(edge_feat_cols) == 0:\n",
    "    raise ValueError(\"엣지 피처가 비었습니다.\")\n",
    "print(f\"[INFO] Using {len(edge_feat_cols)} edge features. First 12: {edge_feat_cols[:12]}\")\n",
    "\n",
    "# =========================\n",
    "# 3) Split / Scale (Group by pair)\n",
    "# =========================\n",
    "y_raw = df[\"risk_score\"].astype(float).values\n",
    "labeled_mask_np = ~np.isnan(y_raw)\n",
    "labeled_idx = np.where(labeled_mask_np)[0]\n",
    "\n",
    "lab_train, lab_val, lab_test = group_splits_by_pair(\n",
    "    df, labeled_idx, test_size=TEST_SIZE, val_size=VAL_SIZE, seed=SEED\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "if len(edge_num_cols) > 0:\n",
    "    scaler.fit(df.loc[lab_train, edge_num_cols].astype(float))\n",
    "    df[edge_num_cols] = scaler.transform(df[edge_num_cols].astype(float))\n",
    "\n",
    "# =========================\n",
    "# 4) Graph build (bidirectional)\n",
    "# =========================\n",
    "all_nodes = pd.Index(pd.unique(pd.concat([df[\"u\"], df[\"v\"]])))\n",
    "node_to_idx = {nid:i for i, nid in enumerate(all_nodes)}\n",
    "num_nodes = len(all_nodes)\n",
    "\n",
    "in_deg  = np.zeros(num_nodes, dtype=np.float32)\n",
    "out_deg = np.zeros(num_nodes, dtype=np.float32)\n",
    "for u, v in zip(df[\"u\"].values, df[\"v\"].values):\n",
    "    ui = node_to_idx[u]; vi = node_to_idx[v]\n",
    "    out_deg[ui] += 1; in_deg[vi]  += 1\n",
    "\n",
    "node_x = np.stack([\n",
    "    in_deg,\n",
    "    out_deg,\n",
    "    in_deg + out_deg,\n",
    "    np.log1p(in_deg),\n",
    "    np.log1p(out_deg),\n",
    "], axis=1).astype(np.float32)\n",
    "\n",
    "u_idx = df[\"u\"].map(node_to_idx).values\n",
    "v_idx = df[\"v\"].map(node_to_idx).values\n",
    "\n",
    "ei     = np.vstack([u_idx, v_idx])\n",
    "ei_rev = np.vstack([v_idx, u_idx])\n",
    "edge_index = torch.tensor(np.hstack([ei, ei_rev]), dtype=torch.long)\n",
    "\n",
    "ea = df[edge_feat_cols].values.astype(np.float32)\n",
    "edge_attr  = torch.tensor(np.vstack([ea, ea]), dtype=torch.float32)\n",
    "x          = torch.tensor(node_x, dtype=torch.float32)\n",
    "\n",
    "E = len(df)  # 원본 엣지 수\n",
    "\n",
    "# labels (log-space), mask (E 기준)\n",
    "y_t_np = np.where(np.isnan(y_raw), np.nan, y_to_t(y_raw))\n",
    "y_t_filled = np.where(np.isnan(y_t_np), 0.0, y_t_np)\n",
    "y_t = torch.tensor(y_t_filled, dtype=torch.float32).view(-1,1)\n",
    "labeled_mask = torch.tensor(labeled_mask_np, dtype=torch.bool)\n",
    "\n",
    "edge_train_mask = torch.zeros(E, dtype=torch.bool); edge_train_mask[lab_train] = True\n",
    "edge_val_mask   = torch.zeros(E, dtype=torch.bool); edge_val_mask[lab_val]   = True\n",
    "edge_test_mask  = torch.zeros(E, dtype=torch.bool); edge_test_mask[lab_test] = True\n",
    "\n",
    "data = Data(\n",
    "    x=x, edge_index=edge_index, edge_attr=edge_attr, y=y_t,\n",
    "    edge_train_mask=edge_train_mask,\n",
    "    edge_val_mask=edge_val_mask,\n",
    "    edge_test_mask=edge_test_mask,\n",
    "    labeled_mask=labeled_mask\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"[INFO] nodes={num_nodes:,}, edges_dir={edge_index.size(1):,} (2E), E(original)={E:,}, labeled={labeled_mask_np.sum():,} \"\n",
    "      f\"(train/val/test = {len(lab_train)}/{len(lab_val)}/{len(lab_test)})\")\n",
    "\n",
    "# =========================\n",
    "# 4.5) HI_THR from labeled\n",
    "# =========================\n",
    "y_labeled = df[\"risk_score\"].dropna().values\n",
    "HI_THR  = float(np.quantile(y_labeled, HI_PCTILE))   # e.g., p85\n",
    "print(f\"[INFO] HI_THR (p{int(HI_PCTILE*100)}) = {HI_THR:.6f}, HI_MULT = {HI_MULT}\")\n",
    "\n",
    "# =========================\n",
    "# 5) Model\n",
    "# =========================\n",
    "class EdgeRegressor(nn.Module):\n",
    "    def __init__(self, in_node_dim, in_edge_dim):\n",
    "        super().__init__()\n",
    "        self.gats = nn.ModuleList()\n",
    "        last = in_node_dim\n",
    "        for _ in range(GAT_LAYERS):\n",
    "            self.gats.append(\n",
    "                GATv2Conv(\n",
    "                    in_channels=last,\n",
    "                    out_channels=GAT_HIDDEN,\n",
    "                    heads=GAT_HEADS,\n",
    "                    dropout=GAT_DROPOUT,\n",
    "                    share_weights=True,\n",
    "                    edge_dim=in_edge_dim\n",
    "                )\n",
    "            )\n",
    "            last = GAT_HIDDEN * GAT_HEADS\n",
    "        in_mlp = last*2 + in_edge_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_mlp, MLP_HIDDEN),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(MLP_DROPOUT),\n",
    "            nn.Linear(MLP_HIDDEN, MLP_HIDDEN//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(MLP_DROPOUT),\n",
    "            nn.Linear(MLP_HIDDEN//2, 1)\n",
    "        )\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        z = x\n",
    "        for conv in self.gats:\n",
    "            z = conv(z, edge_index, edge_attr=edge_attr)\n",
    "            z = F.elu(z)\n",
    "            z = F.dropout(z, p=GAT_DROPOUT, training=self.training)\n",
    "        src, dst = edge_index\n",
    "        zu = z[src]; zv = z[dst]\n",
    "        h = torch.cat([zu, zv, edge_attr], dim=1)\n",
    "        out = self.mlp(h).squeeze(1)  # log-target\n",
    "        return out\n",
    "\n",
    "model = EdgeRegressor(\n",
    "    in_node_dim=data.x.size(1),\n",
    "    in_edge_dim=data.edge_attr.size(1)\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.SmoothL1Loss(reduction=\"none\")  # Huber\n",
    "\n",
    "# ----------- 가중치 -----------\n",
    "len_w = torch.tensor(df[\"length\"].fillna(1).values, dtype=torch.float32, device=DEVICE) if \"length\" in df.columns else torch.ones(E, device=DEVICE)\n",
    "len_w = len_w / (len_w.mean() + 1e-8)\n",
    "\n",
    "yt_log = data.y.view(-1)\n",
    "sig_w = (1.0 + 0.5 * (yt_log[:E] > math.log1p(0.001*Y_SCALE))).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_orig_np = t_to_y(yt_log[:E].detach().cpu().numpy())\n",
    "hi_mask_np = (y_orig_np >= HI_THR)\n",
    "hi_w = np.where(hi_mask_np, HI_MULT, 1.0).astype(np.float32)\n",
    "hi_w = torch.tensor(hi_w, device=DEVICE)\n",
    "\n",
    "base_w = (LEN_W*len_w + SIG_W*sig_w + HI_W*hi_w).clamp_(W_MIN, W_MAX)\n",
    "\n",
    "# =========================\n",
    "# Eval helpers (raw, no calib during training)\n",
    "# =========================\n",
    "def eval_split(mask: torch.Tensor, hi_only=False):\n",
    "    mask = (mask & data.labeled_mask)\n",
    "    if mask.sum() == 0:\n",
    "        return {\"R2\": float(\"nan\"), \"RMSE\": float(\"nan\"), \"MAE\": float(\"nan\"), \"MAPE%\": float(\"nan\")}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_t_all = model(data.x, data.edge_index, data.edge_attr)\n",
    "        pred_t = pred_t_all[:E]\n",
    "        yp = t_to_y(pred_t.detach().cpu().numpy())\n",
    "        yt_true = t_to_y(data.y.detach().cpu().numpy().ravel()[:E])\n",
    "        idx = mask.detach().cpu().numpy()\n",
    "        yp = yp[idx]; yt_true = yt_true[idx]\n",
    "        if hi_only:\n",
    "            hmask = yt_true >= HI_THR\n",
    "            if hmask.sum() == 0:\n",
    "                return {\"R2\": float(\"nan\"), \"RMSE\": float(\"nan\"), \"MAE\": float(\"nan\"), \"MAPE%\": float(\"nan\")}\n",
    "            yp, yt_true = yp[hmask], yt_true[hmask]\n",
    "        return metrics(yt_true, yp)\n",
    "\n",
    "def val_objective():\n",
    "    \"\"\"RMSE_all + coef*RMSE_hi (HI 표본이 적으면 coef 완화)\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pt = model(data.x, data.edge_index, data.edge_attr)[:E].detach().cpu().numpy()\n",
    "        yp_all = t_to_y(pt)\n",
    "        yt_all = t_to_y(data.y.detach().cpu().numpy().ravel()[:E])\n",
    "        m_val  = (data.edge_val_mask & data.labeled_mask).cpu().numpy()\n",
    "        yp, yt = yp_all[m_val], yt_all[m_val]\n",
    "        rmse_all = math.sqrt(mean_squared_error(yt, yp))\n",
    "        m_hi = yt >= HI_THR\n",
    "        hi_cnt = int(m_hi.sum())\n",
    "        coef = 0.15 if hi_cnt < 25 else 0.30\n",
    "        rmse_hi = math.sqrt(mean_squared_error(yt[m_hi], yp[m_hi])) if hi_cnt > 0 else rmse_all\n",
    "        return rmse_all + coef*rmse_hi\n",
    "\n",
    "# =========================\n",
    "# 6) Train\n",
    "# =========================\n",
    "best_obj = float(\"inf\")\n",
    "best_state = None\n",
    "pat = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred_t_all = model(data.x, data.edge_index, data.edge_attr)\n",
    "    pred_t = pred_t_all[:E]\n",
    "    mask = (data.edge_train_mask & data.labeled_mask)\n",
    "\n",
    "    # 동적 잔차 가중\n",
    "    pred_y_all_t = t_to_y_torch(pred_t)\n",
    "    true_y_all_t = t_to_y_torch(data.y.view(-1)[:E])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_y_np  = pred_y_all_t.detach().cpu().numpy()\n",
    "        true_y_np  = true_y_all_t.detach().cpu().numpy()\n",
    "        resid_np   = np.zeros_like(pred_y_np, dtype=np.float32)\n",
    "        m_np       = mask.cpu().numpy()\n",
    "        resid_np[m_np] = np.abs(pred_y_np[m_np] - true_y_np[m_np])\n",
    "    resid = torch.tensor(resid_np, device=DEVICE)\n",
    "    res_w = (1.0 + RES_ALPHA * (resid ** RES_BETA)).clamp_(*RES_CLIP_MINMAX)\n",
    "    w = (base_w * res_w).clamp_(W_MIN, W_MAX)\n",
    "\n",
    "    # 손실 (log-Huber + quantile 혼합)\n",
    "    loss_elem = criterion(pred_t[mask], data.y.view(-1)[:E][mask])\n",
    "    q_loss = quantile_loss(true_y_all_t[mask], pred_y_all_t[mask], tau=QL_TAU)\n",
    "    loss = (loss_elem * w[mask]).mean() * (1.0 - QL_LAMBDA) + q_loss * QL_LAMBDA\n",
    "\n",
    "    loss.backward(); optimizer.step()\n",
    "\n",
    "    if epoch % PRINT_EVERY == 1 or epoch == EPOCHS:\n",
    "        tr_all = eval_split(data.edge_train_mask)\n",
    "        va_all = eval_split(data.edge_val_mask)\n",
    "        va_hi  = eval_split(data.edge_val_mask, hi_only=True)\n",
    "        bar = progress_bar(epoch, EPOCHS, 40)\n",
    "        print(f\"{bar} | loss={loss.item():.6f} | \"\n",
    "              f\"TRN R2={tr_all['R2']:.4f} MAE={tr_all['MAE']:.6f} RMSE={tr_all['RMSE']:.6f} | \"\n",
    "              f\"VAL R2={va_all['R2']:.4f} MAE={va_all['MAE']:.6f} RMSE={va_all['RMSE']:.6f} | \"\n",
    "              f\"VAL(hi) R2={va_hi['R2']:.4f} RMSE={va_hi['RMSE']:.6f}\")\n",
    "\n",
    "    obj = val_objective()\n",
    "    if obj + 1e-9 < best_obj:\n",
    "        best_obj = obj\n",
    "        best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "        pat = 0\n",
    "    else:\n",
    "        pat += 1\n",
    "        if pat >= PATIENCE:\n",
    "            print(f\"[Early Stop] epoch={epoch}, best Val Objective={best_obj:.6f}\")\n",
    "            break\n",
    "\n",
    "# =========================\n",
    "# 7) CV Calibration (+Smooth Blend) with safety switch\n",
    "# =========================\n",
    "if best_state is not None:\n",
    "    model.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pt_full = model(data.x, data.edge_index, data.edge_attr).detach().cpu().numpy().ravel()\n",
    "pt_all = pt_full[:E]\n",
    "tt_all = data.y.detach().cpu().numpy().ravel()[:E]\n",
    "\n",
    "def _t2y(x): return np.clip(np.expm1(x)/Y_SCALE, 0.0, 1.0)\n",
    "\n",
    "# K-Fold on TRAIN edges to learn calibration\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "tr_mask_np = (data.edge_train_mask & data.labeled_mask).cpu().numpy()\n",
    "idx_tr = np.where(tr_mask_np)[0]\n",
    "pt_tr = pt_all[idx_tr]\n",
    "tt_tr = tt_all[idx_tr]\n",
    "\n",
    "A_list, B_list, iso_list, w_list = [], [], [], []\n",
    "for tr_idx, va_idx in kf.split(pt_tr):\n",
    "    pt_tr_tr, pt_tr_va = pt_tr[tr_idx], pt_tr[va_idx]\n",
    "    tt_tr_tr, tt_tr_va = tt_tr[tr_idx], tt_tr[va_idx]\n",
    "\n",
    "    low_mask = _t2y(tt_tr_tr) < HI_THR\n",
    "    if low_mask.sum() >= 2:\n",
    "        lin = LinearRegression().fit(pt_tr_tr[low_mask].reshape(-1,1), tt_tr_tr[low_mask])\n",
    "        A_cv, B_cv = float(lin.coef_[0]), float(lin.intercept_)\n",
    "    else:\n",
    "        A_cv, B_cv = 1.0, 0.0\n",
    "\n",
    "    iso_cv = IsotonicRegression(out_of_bounds='clip')\n",
    "    high_mask = ~low_mask\n",
    "    if high_mask.sum() >= 2:\n",
    "        iso_cv.fit(_t2y(pt_tr_tr[high_mask]), _t2y(tt_tr_tr[high_mask]))\n",
    "    else:\n",
    "        iso_cv.fit(np.array([0.0,1.0]), np.array([0.0,1.0]))\n",
    "\n",
    "    # local piecewise + smooth blend\n",
    "    def _apply_piecewise_blend_local(pred_t, k=BLEND_K):\n",
    "        y_low  = _t2y(A_cv*pred_t + B_cv)\n",
    "        y_high = iso_cv.transform(_t2y(pred_t))\n",
    "        s = 1.0 / (1.0 + np.exp(-k*(y_low - HI_THR)))  # sigmoid blend\n",
    "        return (1.0 - s)*y_low + s*y_high\n",
    "\n",
    "    yp_va = _apply_piecewise_blend_local(pt_tr_va)\n",
    "    rmse_va = math.sqrt(mean_squared_error(_t2y(tt_tr_va), yp_va))\n",
    "    weight = 1.0 / (rmse_va + 1e-9)\n",
    "\n",
    "    A_list.append(A_cv); B_list.append(B_cv); iso_list.append(iso_cv); w_list.append(weight)\n",
    "\n",
    "A_low = float(np.average(A_list, weights=w_list)) if len(w_list) else 1.0\n",
    "B_low = float(np.average(B_list, weights=w_list)) if len(w_list) else 0.0\n",
    "iso   = iso_list[int(np.argmax(w_list))] if len(w_list) else IsotonicRegression(out_of_bounds='clip').fit([0,1],[0,1])\n",
    "\n",
    "def apply_piecewise_blend(pred_t_vec: np.ndarray, k: float = BLEND_K) -> np.ndarray:\n",
    "    y_low  = _t2y(A_low*pred_t_vec + B_low)\n",
    "    y_high = iso.transform(_t2y(pred_t_vec))\n",
    "    s = 1.0 / (1.0 + np.exp(-k*(y_low - HI_THR)))  # 0~1\n",
    "    return (1.0 - s)*y_low + s*y_high\n",
    "\n",
    "# ---- Compare calibrated vs raw on validation, fallback if needed ----\n",
    "def _eval_rmse_on_val(predict_fn):\n",
    "    with torch.no_grad():\n",
    "        pt = model(data.x, data.edge_index, data.edge_attr).detach().cpu().numpy().ravel()[:E]\n",
    "    yp = predict_fn(pt)\n",
    "    yt = t_to_y(data.y.detach().cpu().numpy().ravel()[:E])\n",
    "    m  = (data.edge_val_mask & data.labeled_mask).cpu().numpy()\n",
    "    return math.sqrt(mean_squared_error(yt[m], yp[m]))\n",
    "\n",
    "rmse_val_raw  = _eval_rmse_on_val(lambda pt: t_to_y(pt))\n",
    "rmse_val_cal  = _eval_rmse_on_val(lambda pt: apply_piecewise_blend(pt, k=BLEND_K))\n",
    "USE_CAL = rmse_val_cal <= rmse_val_raw * 1.001  # 0.1%라도 개선 못하면 raw 사용\n",
    "\n",
    "if not USE_CAL:\n",
    "    print(f\"[CALIB] Disabled (raw better): VAL_RMSE_RAW={rmse_val_raw:.6f} < VAL_RMSE_CAL={rmse_val_cal:.6f}\")\n",
    "else:\n",
    "    print(f\"[CALIB] Enabled: VAL_RMSE_CAL={rmse_val_cal:.6f} <= RAW={rmse_val_raw:.6f}\")\n",
    "\n",
    "def _predict_final(pt):\n",
    "    if USE_CAL:\n",
    "        return apply_piecewise_blend(pt, k=BLEND_K)\n",
    "    else:\n",
    "        return t_to_y(pt)\n",
    "\n",
    "# =========================\n",
    "# 8) Final eval (calib switch applied) & save\n",
    "# =========================\n",
    "def evaluate_final(split_mask, hi_only=False):\n",
    "    mask = (split_mask & data.labeled_mask).cpu().numpy()\n",
    "    if mask.sum() == 0:\n",
    "        return {\"R2\": float(\"nan\"), \"RMSE\": float(\"nan\"), \"MAE\": float(\"nan\"), \"MAPE%\": float(\"nan\")}\n",
    "    with torch.no_grad():\n",
    "        pt_full_eval = model(data.x, data.edge_index, data.edge_attr).detach().cpu().numpy().ravel()\n",
    "        pt = pt_full_eval[:E]\n",
    "        yp = _predict_final(pt)\n",
    "        yt = t_to_y(data.y.detach().cpu().numpy().ravel()[:E])\n",
    "    yp, yt = yp[mask], yt[mask]\n",
    "    if hi_only:\n",
    "        m = yt >= HI_THR\n",
    "        if m.sum() == 0: return {\"R2\": float(\"nan\"), \"RMSE\": float(\"nan\"), \"MAE\": float(\"nan\"), \"MAPE%\": float(\"nan\")}\n",
    "        yp, yt = yp[m], yt[m]\n",
    "    return metrics(yt, yp)\n",
    "\n",
    "tr   = evaluate_final(data.edge_train_mask)\n",
    "va   = evaluate_final(data.edge_val_mask)\n",
    "te   = evaluate_final(data.edge_test_mask)\n",
    "va_h = evaluate_final(data.edge_val_mask, hi_only=True)\n",
    "te_h = evaluate_final(data.edge_test_mask, hi_only=True)\n",
    "\n",
    "print(\"\\n== Final Metrics (original scale, calibrated if helpful) ==\")\n",
    "print(f\"Train: R2={tr['R2']:.4f} | RMSE={tr['RMSE']:.6f} | MAE={tr['MAE']:.6f}\")\n",
    "print(f\"Valid: R2={va['R2']:.4f} | RMSE={va['RMSE']:.6f} | MAE={va['MAE']:.6f}\")\n",
    "print(f\" Test: R2={te['R2']:.4f} | RMSE={te['RMSE']:.6f} | MAE={te['MAE']:.6f}\")\n",
    "print(f\"[High-risk ≥ {HI_THR:.6f}]  Valid: R2={va_h['R2']:.4f} RMSE={va_h['RMSE']:.6f} MAE={va_h['MAE']:.6f} | \"\n",
    "      f\"Test: R2={te_h['R2']:.4f} RMSE={te_h['RMSE']:.6f} MAE={te_h['MAE']:.6f}\")\n",
    "\n",
    "# 전체 예측 저장\n",
    "with torch.no_grad():\n",
    "    pred_all_t_full = model(data.x, data.edge_index, data.edge_attr).detach().cpu().numpy().ravel()\n",
    "    pred_all_t = pred_all_t_full[:E]\n",
    "    pred_all   = _predict_final(pred_all_t)\n",
    "\n",
    "out_df = df[[\"u\",\"v\"]].copy()\n",
    "out_df[\"risk_score_true\"] = df[\"risk_score\"].values\n",
    "out_df[\"risk_score_pred\"] = pred_all\n",
    "save_pred = os.path.join(OUT_DIR, \"gat_edge_predictions_fullgraph_v4.csv\")\n",
    "out_df.to_csv(save_pred, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[SAVE] {save_pred}\")\n",
    "\n",
    "# 모델 저장\n",
    "ckpt_path = os.path.join(OUT_DIR, \"gat_edge_regressor_fullgraph_v4.pt\")\n",
    "torch.save({\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"config\": {\n",
    "        \"EPOCHS\": EPOCHS, \"H\": GAT_HIDDEN, \"HEADS\": GAT_HEADS,\n",
    "        \"LAYERS\": GAT_LAYERS, \"DROPOUT\": GAT_DROPOUT,\n",
    "        \"MLP_HIDDEN\": MLP_HIDDEN, \"MLP_DROPOUT\": MLP_DROPOUT,\n",
    "        \"Y_SCALE\": Y_SCALE,\n",
    "        \"HI_THR\": HI_THR, \"HI_PCTILE\": HI_PCTILE, \"HI_MULT\": HI_MULT,\n",
    "        \"RES_ALPHA\": RES_ALPHA, \"RES_BETA\": RES_BETA,\n",
    "        \"QL_TAU\": QL_TAU, \"QL_LAMBDA\": QL_LAMBDA,\n",
    "        \"CALIB\": \"CV piecewise + smooth blend\",\n",
    "        \"BLEND_K\": BLEND_K,\n",
    "        \"USE_CAL\": bool(USE_CAL),\n",
    "    },\n",
    "    \"edge_feat_cols\": edge_feat_cols,\n",
    "    \"node_feat_dim\": int(data.x.size(1)),\n",
    "    \"edge_feat_dim\": int(data.edge_attr.size(1)),\n",
    "    \"scaler_mean_\": scaler.mean_.tolist() if hasattr(scaler, \"mean_\") else None,\n",
    "    \"scaler_scale_\": scaler.scale_.tolist() if hasattr(scaler, \"scale_\") else None,\n",
    "    \"lin_low\": {\"A\": float(A_low), \"B\": float(B_low)} if 'A_low' in locals() else {\"A\": 1.0, \"B\": 0.0}\n",
    "}, ckpt_path)\n",
    "print(f\"[SAVE] {ckpt_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
